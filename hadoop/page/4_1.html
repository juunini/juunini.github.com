<h1><strong><color4>주의</color4> : <color2>Part. 4는 하둡2와 3의 실패예제입니다. 학습 목적으로 오셨다면 참고하지 마세요. Part. 5로 넘어가세요.</color2></strong></h1><br>
<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>
Hadoop 2를 시작합니다!<br>
아....니지. 저는 <color2>Hadoop 3</color2>을 시작합니다!<br>
<br>
2017년 11월에 Hadoop3 안정화 버전이 출시되었습니다.<br>
책에는 Hadoop2가 나와있지만 이미 3이 나온 이상 언제까지나 2일 순 없습니다.<br>
그래서 저는 <color3>Hadoop 3</color3>으로 실습을 진행하겠습니다.<br>
<br>
실습 환경은<br>

<terminal>
HDD : 100GB
메모리 : 6GB
CPU : 쿼드코어
</terminal>

이렇게 설정하고 진행하겠습니다.<br>
하나만 만들 것이구요, 한 대의 노드에 모든 데몬을 실행하는 가상 분산 모드로 설치를 진행하겠습니다.<br>
만약 필요하다면 노드를 늘릴 수 있도록 현재 만드는 머신의 이름은 <color2>node1</color2> 이라고 짓겠습니다.<br>
<br>
자, 그러면 저는 이 전에 쓰던 하둡 노드들을 전부 삭제해버리고 새로 만들어서 설치하겠습니다.<br>
<br>
<blur>(설치 중...)</blur><br>
<br>
<terminal>
<color4>yum repolist && yum update -y && yum install -y net-tools openssh* openssl* epel-release R wget gcc-c++ pdsh im-chooser && yum groupinstall -y "X Window System" "Cinnamon Desktop" && systemctl set-default graphical.target && systemctl isolate graphical.target</color4>
</terminal>

한!방!설!치!
<br>
아, 그리고 이번에는 고정IP 설정을 할겁니다.<br>
실습하다가 한번씩 IP가 바뀌는 짜증나는 상황이 발생해서 한번 짚고 넘어갈까 합니다.<br>
Centos 버전마다 조금씩 하는 방법이 바뀌었는지 인터넷에는 안되는 방법들이 수두룩했는데 결국 찾아냈습니다.<br>
<br>
<color4>/etc/sysconfig/network-scripts/ifcfg-ens32</color4>

<terminal>
TYPE="Ethernet"
PROXY_METHOD="none"
BROWSER_ONLY="no"
BOOTPROTO="none"
DEFROUTE="yes"
IPV4_FAILURE_FATAL="no"
IPV6INIT="no"
IPV6_AUTOCONF="yes"
IPV6_DEFROUTE="yes"
IPV6_FAILURE_FATAL="no"
IPV6_ADDR_GEN_MODE="stable-privacy"
NAME="ens32"
UUID="37bff84e-3c3e-432b-a781-422034bc264f"
DEVICE="ens32"
ONBOOT="yes"
IPADDR="192.168.111.100"
NETMASK="255.255.255.0"
GATEWAY="192.168.111.2"
DNS1="192.168.111.2"
DNS2="8.8.8.8"
</terminal>

그 뒤에 <color4>sudo service network restart</color4> 를 입력하면 IP가 고정이 됩니다.<br>
192.168.<color4>111</color4>.100 에서 <color2>111</color2>은 제가 설정한 부분이기 때문에<br>
여러분은 다를 수도 있습니다. 잘 모르겠으면 자세한건 검색을 하세요.<br>
<br>
고정IP도 설정했고, 이번에는 openjdk 말고 자바를 직접 최신버전으로 다운로드 받았습니다.<br>
<color2>jdk-9.0.4</color2>를 다운로드 받았고 경로는 <color2>/home/hadoop</color2> 그러니까 유저의 기본 경로에 설정했습니다.<br>
그리고 환경변수에 추가합니다.<br>
<br>
<color2>export PATH=/home/hadoop/jdk-9.0.4/bin:$PATH</color2><br>
<br>
CMake도 설치합니다.<br>

<terminal>
<strong>[hadoop@node1 Download]$</strong> tar -zxvf cmake-3.10.2.tar.gz && rm cmake-3.10.2.tar.gz && cd cmake-3.10.2 && ./bootstrap && make && sudo make install
</terminal>

CMake를 설치한 이유는 아래에 언급할 <color2>protobuf</color2>를 설치하기 위함입니다.<br>
Hadoop2 부터는 <color2>protobuf</color2> 라는게 필요하다는군요. 다운로드 받아서 설치합니다.<br>

<terminal>
<strong>[hadoop@node1 protobuf-3.5.1]$</strong> ./configure && make && sudo make install
</terminal>

<color2>protocol buffer</color2>는 구조화된 데이터를 직렬화 하는 방식, 규약입니다. 구조화 된 데이터란 우리가 쉽게 접하는 텍스트나 멀티미디어 등을 나타내며<br>
그런 구조화된 데이터를 생으로 분석하려면 시간이 오래걸리고 비효율적이기 때문에 데이터를 직렬화시켜 빠르게 읽어들이기 위한 수단입니다.<br>
<br>
다 됐으면 <color2>Hadoop3</color2>를 다운로드 받고 압축을 풉니다.<br>
1.2.1 버전과는 다르게 환경설정 파일들의 경로가 <color2>etc/hadoop</color2>에 있습니다.<br>
<br>
먼저 <color2>hadoop-env.sh</color2> 부터 설정하겠습니다.<br>
전 버전과는 다르게 안에 엄청나게 많은 텍스트가 있다는게 보입니다.<br>
어차피 여기서 수정할건 저번 버전과 같습니다.<br>
<color2>export JAVA_HOME</color2>, <color2>export HADOOP_PID_DIR</color2><br>
그리고 <color2>export HADOOP_HOME_WARN_SUPPRESS="TRUE"</color2> 추가. 끝입니다.<br>
그 다음은 <color2>masters</color2> 파일과 <color2>slaves</color2> 파일을 추가해야 합니다. 없으니 만들어야 하죠.<br>
거기에 전부 <color4>node1</color4> 이라고 써넣었습니다.<br>
<br>
하고나니 왠지 뻘짓같지만 혹시 몰라서 <color2>ssh-keygen</color2> 하고나서 <color2>ssh-copy-id -i node1</color2> 하고 넣어줬습니다.<br>
나 자신한테 나의 공개키를 공유하는건데... 쩝.... 혹시 모르니까요.<br>
<color2>/etc/hosts</color2> 수정하는거 잊지 마시구요.<br>
<br>
<color4>core-site.xml</color4> 파일을 수정할 차례입니다.<br>
<br>
<a href="https://github.com/juunini/juunini.github.com/tree/master/hadoop/img4/core-site.xml" target="_new" class="link">core-site.xml 코드 보러가기</a><br>
<br>
<color2>fs.default.name</color2> 속성의 명칭이 <color2>fs.defaultFS</color2> 으로 변경됐다네요.<br>
다음은 <color4>hdfs-site.xml</color4> 파일을 수정할겁니다. 하는김에 튜닝도 같이 하겠습니다.<br>
<br>
<a href="https://github.com/juunini/juunini.github.com/tree/master/hadoop/img4/hdfs-site.xml" target="_new" class="link">hdfs-site.xml 코드 보러가기</a><br>
<br>
여기도 변한게 많네요.<br>
<color2>dfs.namenode.name.dir</color2>는 파일 시스템의 경로라고 합니다. HDFS에서 파일이 어디 저장되어있는지 알려주는 <color4>지도</color4>같은 거라고 볼 수 있겠네요.<br>
<color2>dfs.namenode.checkpoint.dir</color2>는 보조네임노드가 지도를 저장하는 경로라고 하네요.<br>
<color2>dfs.datanode.data.dir</color2>은 HDFS에서 파일이 저장되는 위치입니다.<br>
나머지는 친숙하시죠? 넘어가겠습니다.<br>
<br>
다음은 <color4>mapred-site.xml</color4>을 수정하겠습니다.<br>
<br>
<a href="https://github.com/juunini/juunini.github.com/tree/master/hadoop/img4/mapred-site.xml" target="_new" class="link">mapred-site.xml 코드 보러가기</a><br>
<br>

<color2>yarn</color2> 이라는 단어가 등장합니다. yarn은 맵리듀스의 잡트래커를 효율적으로 관리하는 역할을 합니다.<br>
<color2>yarn</color2>이 등장한 이후로 맵리듀스의 설정파일이 엄청나게 많이 바뀌었네요.<br>
<br>
자세한건 <color4>yarn-env.sh</color4> 파일과 <color4>yarn-site.xml</color4> 파일을 수정하고나서 이야기하죠.<br>
<br>
<a href="https://github.com/juunini/juunini.github.com/tree/master/hadoop/img4/yarn-env.sh" target="_new" class="link">yarn-env.sh 코드 보러가기</a><br>
<a href="https://github.com/juunini/juunini.github.com/tree/master/hadoop/img4/yarn-site.xml" target="_new" class="link">yarn-site.xml 코드 보러가기</a><br>
<br>
<color2>yarn-env.sh</color2>에 주석을 지운 부분에는 전부 숫자 6000이 들어가있습니다.<br>
이 의미는 힙 메모리를 6GB 할당하겠다는 의미입니다.<br>
힙 메모리가 뭔가 잘 몰라서 찾아보니 <color4>동적 메모리 할당</color4> 이라고 하는데, C나 자바같은 정적언어에서 큰 크기의 데이터를 담고자<br>
추가적으로 할당되는 메모리 영역이라고 합니다.<br>
말을 들어보니 크면 클 수록 좋겠지만, 힙 메모리만큼 운영체제나 다른 프로그램이 그만큼의 메모리를 사용하지 못하게 한다고 합니다.<br>
무조건 크게 잡기 보다는 자신의 메모리 환경에 맞게 구성하는게 좋을 것 같네요. 그래서 저는 6GB로 설정했습니다.<br>
<br>
그리고 <color2>yarn-site.xml</color2>에는 모르는 것 투성이입니다.<br>
그래도 열심히 주석을 달아서 설명해놓았습니다.<br>
이것저것 정신없이 했네요. 그런데 이와중에 <color2>kms-site.xml</color2>이라는 처음보는게 생겨있습니다.<br>
키 암호화 관리자 라는데 음... 아직 무슨 기능인지는 정확히 모르겠습니다.<br>
일단 넘어가도록 하죠.<br>
하둡을 실행시켜보겠습니다.<br>

<terminal>
<strong>[hadoop@node1 hadoop-3.0.0]$</strong> ./bin/hdfs namenode -format
WARNING: /home/hadoop/hadoop-3.0.0/pids does not exist. Creating.
WARNING: /home/hadoop/hadoop-3.0.0/logs does not exist. Creating.
2018-02-02 09:31:30,472 INFO namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = node1/192.168.247.100
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 3.0.0
STARTUP_MSG:   classpath = /home/hadoop/hadoop-3.0.0/etc/hadoop:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/home/hadoop/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z
STARTUP_MSG:   java = 9.0.4
************************************************************/
2018-02-02 09:31:30,479 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-02-02 09:31:30,484 INFO namenode.NameNode: createNameNode [-format]
2018-02-02 09:31:31,052 WARN common.Util: Path /home/hadoop/data/dfs/namenode should be specified as a URI in configuration files. Please update hdfs configuration.
2018-02-02 09:31:31,052 WARN common.Util: Path /home/hadoop/data/dfs/namenode should be specified as a URI in configuration files. Please update hdfs configuration.
Formatting using clusterid: CID-80178b34-6ad3-4633-98de-e4d9be5aba9a
2018-02-02 09:31:31,138 INFO namenode.FSEditLog: Edit logging is async:true
2018-02-02 09:31:31,164 INFO namenode.FSNamesystem: KeyProvider: null
2018-02-02 09:31:31,166 INFO namenode.FSNamesystem: fsLock is fair: true
2018-02-02 09:31:31,168 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2018-02-02 09:31:31,225 INFO namenode.FSNamesystem: fsOwner             = hadoop (auth:SIMPLE)
2018-02-02 09:31:31,225 INFO namenode.FSNamesystem: supergroup          = supergroup
2018-02-02 09:31:31,225 INFO namenode.FSNamesystem: isPermissionEnabled = true
2018-02-02 09:31:31,225 INFO namenode.FSNamesystem: HA Enabled: false
2018-02-02 09:31:31,309 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2018-02-02 09:31:31,335 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2018-02-02 09:31:31,335 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2018-02-02 09:31:31,355 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2018-02-02 09:31:31,356 INFO blockmanagement.BlockManager: The block deletion will start around 2018 2월 02 09:31:31
2018-02-02 09:31:31,358 INFO util.GSet: Computing capacity for map BlocksMap
2018-02-02 09:31:31,358 INFO util.GSet: VM type       = 64-bit
2018-02-02 09:31:31,360 INFO util.GSet: 2.0% max memory 2.9 GB = 59.2 MB
2018-02-02 09:31:31,360 INFO util.GSet: capacity      = 2^23 = 8388608 entries
2018-02-02 09:31:31,640 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false
2018-02-02 09:31:31,658 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2018-02-02 09:31:31,658 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2018-02-02 09:31:31,658 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2018-02-02 09:31:31,658 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2018-02-02 09:31:31,658 INFO blockmanagement.BlockManager: defaultReplication         = 1
2018-02-02 09:31:31,658 INFO blockmanagement.BlockManager: maxReplication             = 512
2018-02-02 09:31:31,658 INFO blockmanagement.BlockManager: minReplication             = 1
2018-02-02 09:31:31,659 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
2018-02-02 09:31:31,659 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
2018-02-02 09:31:31,659 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
2018-02-02 09:31:31,659 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2018-02-02 09:31:31,914 INFO util.GSet: Computing capacity for map INodeMap
2018-02-02 09:31:31,914 INFO util.GSet: VM type       = 64-bit
2018-02-02 09:31:31,914 INFO util.GSet: 1.0% max memory 2.9 GB = 29.6 MB
2018-02-02 09:31:31,914 INFO util.GSet: capacity      = 2^22 = 4194304 entries
2018-02-02 09:31:32,148 INFO namenode.FSDirectory: ACLs enabled? false
2018-02-02 09:31:32,149 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true
2018-02-02 09:31:32,149 INFO namenode.FSDirectory: XAttrs enabled? true
2018-02-02 09:31:32,149 INFO namenode.NameNode: Caching file names occurring more than 10 times
2018-02-02 09:31:32,153 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true
2018-02-02 09:31:32,172 INFO util.GSet: Computing capacity for map cachedBlocks
2018-02-02 09:31:32,172 INFO util.GSet: VM type       = 64-bit
2018-02-02 09:31:32,172 INFO util.GSet: 0.25% max memory 2.9 GB = 7.4 MB
2018-02-02 09:31:32,172 INFO util.GSet: capacity      = 2^20 = 1048576 entries
2018-02-02 09:31:32,218 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2018-02-02 09:31:32,218 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2018-02-02 09:31:32,218 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2018-02-02 09:31:32,237 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
2018-02-02 09:31:32,237 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2018-02-02 09:31:32,238 INFO util.GSet: Computing capacity for map NameNodeRetryCache
2018-02-02 09:31:32,238 INFO util.GSet: VM type       = 64-bit
2018-02-02 09:31:32,238 INFO util.GSet: 0.029999999329447746% max memory 2.9 GB = 909.3 KB
2018-02-02 09:31:32,238 INFO util.GSet: capacity      = 2^17 = 131072 entries
2018-02-02 09:31:32,290 INFO namenode.FSImage: Allocated new BlockPoolId: BP-794599243-192.168.247.100-1517581892284
2018-02-02 09:31:32,324 INFO common.Storage: Storage directory /home/hadoop/data/dfs/namenode has been successfully formatted.
2018-02-02 09:31:32,350 INFO namenode.FSImageFormatProtobuf: Saving image file /home/hadoop/data/dfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression
2018-02-02 09:31:32,522 INFO namenode.FSImageFormatProtobuf: Image file /home/hadoop/data/dfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 391 bytes saved in 0 seconds.
2018-02-02 09:31:32,546 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
2018-02-02 09:31:32,553 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at node1/192.168.247.100
************************************************************/
</terminal>

오우야... 뭔가 정신없이 엄청나게 떠있네요;;<br>
명령어도 약간 바뀐 것 같습니다. hadoop에서 hdfs로요.<br>

<terminal>
<strong>[hadoop@node1 hadoop-3.0.0]$</strong> ./sbin/start-all.sh
WARNING: Attempting to start all Apache Hadoop daemons as hadoop in 10 seconds.
WARNING: This is not a recommended production deployment configuration.
WARNING: Use CTRL-C to abort.
</terminal>

??<br>
실패인가? <key>Ctrl</key> - <key>C</key> 해서 끄라고 했는데 한번 기다려봤습니다.<br>
그 사이에 자바 버전을 하나 낮은걸로 바꿔보았습니다.<br>

<terminal>
<strong>[hadoop@node1 hadoop-3.0.0]$</strong> ./sbin/start-all.sh
WARNING: Attempting to start all Apache Hadoop daemons as hadoop in 10 seconds.
WARNING: This is not a recommended production deployment configuration.
WARNING: Use CTRL-C to abort.
Starting namenodes on [node1]
Starting datanodes
Starting secondary namenodes [node1]
Starting resourcemanager
Starting nodemanagers
<strong>[hadoop@node1 hadoop-3.0.0]$</strong> jps
63424 WebAppProxyServer
62641 SecondaryNameNode
63173 NodeManager
62873 ResourceManager
62442 DataNode
62283 NameNode
63469 Jps
</terminal>

<img src="/hadoop/img4/1_1.jpg" alt="">

어라? 되네? <color4>ERROR</color4>가 아니라 <color4>WARNING</color4>인 경우에는 이런식으로 한번씩 버텨보는데<br>
음.... 작동을 하긴 하네요. 전 버전보다 모니터링이 훨씬 세련되게 바뀐 느낌입니다.<br>
다시 자바 최신버전으로 바꾸고 해도 잘 작동하네요.<br>

<terminal>
<strong>[hadoop@node1 hadoop-3.0.0]$</strong> ./sbin/yarn-daemon.sh start proxyserver
WARNING: Use of this script to start YARN daemons is deprecated.
WARNING: Attempting to execute replacement "yarn --daemon start" instead.
proxyserver is running as process 68250.  Stop it first.
</terminal>

책에서 얀 프록시서버를 켜래서 하랬더니 이미 켜져있다고 그러네요. 그리고 명령어가 비뀐건가봅니다.<br>
<color4>yarn --daemon start</color4> 로 대신해달라고 하네요.<br>
뭐... 됐으니 예제가 잘 실행되는지 한번 실험해보겠습니다.<br>

<terminal>
./bin/hdfs dfs -mkdir /user && ./bin/hdfs dfs -mkdir /user/hadoop && ./bin/hdfs dfs -mkdir /user/hadoop/conf && ./bin/hdfs dfs -put etc/hadoop/hadoop-env.sh conf/ && ./bin/yarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar wordcount conf output

2018-02-02 11:16:33,135 INFO client.RMProxy: Connecting to ResourceManager at node1/192.168.247.100:8032
2018-02-02 11:16:34,674 INFO ipc.Client: Retrying connect to server: node1/192.168.247.100:8032. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-02-02 11:16:35,675 INFO ipc.Client: Retrying connect to server: node1/192.168.247.100:8032. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-02-02 11:16:36,676 INFO ipc.Client: Retrying connect to server: node1/192.168.247.100:8032. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-02-02 11:16:37,678 INFO ipc.Client: Retrying connect to server: node1/192.168.247.100:8032. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-02-02 11:16:38,679 INFO ipc.Client: Retrying connect to server: node1/192.168.247.100:8032. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-02-02 11:16:39,680 INFO ipc.Client: Retrying connect to server: node1/192.168.247.100:8032. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-02-02 11:16:40,680 INFO ipc.Client: Retrying connect to server: node1/192.168.247.100:8032. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2018-02-02 11:16:41,682 INFO ipc.Client: Retrying connect to server: node1/192.168.247.100:8032. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
</terminal>

??<br>
또 뭔가 난관에 봉착했습니다.<br>
아... 리소스매니저가 안켜진건가....<br>
아닌데... jps에 올라와있는데....<br>
<br>
인터넷에 찾아보니 이런 답변이 있군요.<br>

<terminal>
&lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;
    &lt;value&gt;127.0.0.1:8032&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;
    &lt;value&gt;127.0.0.1:8030&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;
    &lt;value&gt;127.0.0.1:8031&lt;/value&gt;
&lt;/property&gt;
</terminal>

추가해도 전혀 도움이 되지 않았습니다. 다음 글에 다시 시도하는걸 보여드리죠.<br>
<br>
<a href="/hadoop/page/4_2.html" onclick="
    event.preventDefault();

    let xhttp = new XMLHttpRequest();

    xhttp.onreadystatechange = () => {
        document.querySelector('main').innerHTML = xhttp.responseText;
    }

    xhttp.open('GET', '/hadoop/page/4_2.html', true);
    xhttp.send();

    document.title = 'Hadoop Guide Part. 4 - Step. 2';
    history.pushState('/hadoop/page/4_2.html' + ' ' + 'Part. 4 - Step. 2', null, '#4_2');

    document.querySelector('side').children[3].classList.add('on');
    document.querySelector('side').children[3].children[1].classList.remove('on');
    document.querySelector('side').children[3].children[2].classList.add('on');
" class="button">다음단계로 가기</a>